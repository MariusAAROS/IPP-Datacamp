{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Decision Trees to Random Forests\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that by increasing the depth of the tree, we are going to get an over-fitted model. A way to bypass the choice of a specific depth it to combine several trees together.\n",
    "\n",
    "Let's start by training several trees on slightly different data. The slightly different dataset could be generated by randomly sampling with replacement. In statistics, this called a boostrap sample. We will use the iris dataset to create such ensemble and ensure that we have some data for training and some left out data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to train several decision trees, we will run a single tree. However, instead to train this tree on `X_train`, we want to train it on a bootstrap sample. You can use the `np.random.choice` function sample with replacement some index. You will need to create a sample_weight vector and pass it to the `fit` method of the `DecisionTreeClassifier`. We provide the `generate_sample_weight` function which will generate the `sample_weight` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_idx(X):\n",
    "    indices = np.random.choice(\n",
    "        np.arange(X.shape[0]), size=X.shape[0], replace=True\n",
    "    )\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37,  65,  12,  86,  30,  71,  15,  79,  75,  49,   5, 101,  76,\n",
       "         2,  84,  69,  10,  57, 107,   8,  25,  73,  47, 103,  89,   8,\n",
       "        47, 107,  72,  46,  31,  41,  25,  78,   6,  79,   3, 109,  27,\n",
       "        66,  26,  55,  37,  40,  95,  59,  12,  35,  73,  23,  10, 109,\n",
       "        12,  71,  32,  89, 110,  65,   4, 100,  20,  89,  36,  61, 106,\n",
       "         5,  99,  48,  45,  26,  35,  46,   9,   9,  98,  16,  99,  84,\n",
       "        81,  31,  74,  20,  58,  67,  99,  11,  28,  79,  88,  95,  99,\n",
       "        58,  67,  65,   9,   4,  39,  51,   4,  92,  63,  27,  11,  24,\n",
       "        82,   6,  79,  50,  50,  25,  53,  95])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_idx(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({28: 4,\n",
       "         48: 3,\n",
       "         50: 3,\n",
       "         54: 3,\n",
       "         26: 3,\n",
       "         34: 3,\n",
       "         65: 3,\n",
       "         18: 3,\n",
       "         57: 3,\n",
       "         105: 2,\n",
       "         35: 2,\n",
       "         24: 2,\n",
       "         96: 2,\n",
       "         94: 2,\n",
       "         67: 2,\n",
       "         84: 2,\n",
       "         36: 2,\n",
       "         31: 2,\n",
       "         49: 2,\n",
       "         66: 2,\n",
       "         5: 2,\n",
       "         37: 2,\n",
       "         82: 2,\n",
       "         97: 2,\n",
       "         71: 2,\n",
       "         11: 2,\n",
       "         74: 2,\n",
       "         107: 2,\n",
       "         87: 2,\n",
       "         13: 1,\n",
       "         52: 1,\n",
       "         33: 1,\n",
       "         81: 1,\n",
       "         53: 1,\n",
       "         83: 1,\n",
       "         46: 1,\n",
       "         92: 1,\n",
       "         70: 1,\n",
       "         23: 1,\n",
       "         17: 1,\n",
       "         39: 1,\n",
       "         77: 1,\n",
       "         98: 1,\n",
       "         68: 1,\n",
       "         80: 1,\n",
       "         111: 1,\n",
       "         91: 1,\n",
       "         64: 1,\n",
       "         43: 1,\n",
       "         79: 1,\n",
       "         14: 1,\n",
       "         20: 1,\n",
       "         104: 1,\n",
       "         78: 1,\n",
       "         72: 1,\n",
       "         59: 1,\n",
       "         93: 1,\n",
       "         75: 1,\n",
       "         15: 1,\n",
       "         76: 1,\n",
       "         22: 1,\n",
       "         32: 1,\n",
       "         106: 1,\n",
       "         2: 1,\n",
       "         42: 1,\n",
       "         19: 1,\n",
       "         90: 1,\n",
       "         45: 1,\n",
       "         7: 1,\n",
       "         110: 1,\n",
       "         6: 1,\n",
       "         29: 1,\n",
       "         103: 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(bootstrap_idx(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    indices = bootstrap_idx(X)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bootstrap, y_train_bootstrap = bootstrap_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes distribution in the original data: Counter({1: 38, 0: 37, 2: 37})\n",
      "Classes distribution in the bootstrap: Counter({0: 44, 1: 41, 2: 27})\n"
     ]
    }
   ],
   "source": [
    "print(f'Classes distribution in the original data: {Counter(y_train)}')\n",
    "print(f'Classes distribution in the bootstrap: {Counter(y_train_bootstrap)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Create a bagging classifier</b>:<br>\n",
    "    <br>\n",
    "    A bagging classifier will train several decision tree classifiers, each of them on a different bootstrap sample.\n",
    "     <ul>\n",
    "      <li>\n",
    "      Create several <code>DecisionTreeClassifier</code> and store them in a Python list;\n",
    "      </li>\n",
    "      <li>\n",
    "      Loop over these trees and <code>fit</code> them by generating a bootstrap sample using <code>bootstrap_sample</code> function;\n",
    "      </li>\n",
    "      <li>\n",
    "      To predict with this ensemble of trees on new data (testing set), you can provide the same set to each tree and call the <code>predict</code> method. Aggregate all predictions in a NumPy array;\n",
    "      </li>\n",
    "      <li>\n",
    "      Once the predictions available, you need to provide a single prediction: you can retain the class which was the most predicted which is called a majority vote;\n",
    "      </li>\n",
    "      <li>\n",
    "      Finally, check the accuracy of your model.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ntrees = 5\n",
    "max_depth = 3\n",
    "\n",
    "tree_list = []\n",
    "ypred_list = []\n",
    "for i in range(ntrees):\n",
    "    X_train_i, y_train_i = bootstrap_sample(X_train, y_train)\n",
    "    treei = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    treei.fit(X_train_i, y_train_i)\n",
    "    ypred_list.append(treei.predict(X_test))\n",
    "    tree_list.append(treei)\n",
    "    \n",
    "ypred,_ = mode(np.array(ypred_list), axis=0, keepdims=False)\n",
    "print(accuracy_score(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: using scikit-learn</b>:\n",
    "    <br>\n",
    "    After implementing your own bagging classifier, use a <code>BaggingClassifier</code> from scikit-learn to fit the above data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=max_depth),\n",
    "    n_estimators=ntrees\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "print(bagging.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very famous classifier is the random forest classifier. It is similar to the bagging classifier. In addition of the bootstrap, the random forest will use a subset of features (selected randomly) to find the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Create a random forest classifier</b>:\n",
    "    <br>\n",
    "    Use your previous code which was generated several <code>DecisionTreeClassifier</code>. Check the list of the option of this classifier and modify one of the parameters such that only the $\\sqrt{F}$ features are used for the splitting. $F$ represents the number of features in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: using scikit-learn</b>:\n",
    "    <br>\n",
    "    After implementing your own random forest classifier, use a <code>RandomForestClassifier</code> from scikit-learn to fit the above data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import plot_forest_interactive\n",
    "plot_forest_interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
